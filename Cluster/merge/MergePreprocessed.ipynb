{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e52f0ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 spark.sql.warehouse.dir = file:/media/johannes/DATASETS/SpotiFeat/spark-warehouse\n",
      "                   spark.executor.memory = 2g                  \n",
      "                       spark.driver.host = gpu01.inf-ra.uni-jena.de\n",
      "                         spark.cores.max = 16                  \n",
      "                       spark.executor.id = driver              \n",
      "                          spark.app.name = 01_geolocation      \n",
      "                            spark.app.id = local-1692616536511 \n",
      "           spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "                      spark.rdd.compress = True                \n",
      "      spark.serializer.objectStreamReset = 100                 \n",
      "                            spark.master = local[*]            \n",
      "                    spark.submit.pyFiles =                     \n",
      "                    spark.executor.cores = 1                   \n",
      "                 spark.submit.deployMode = client              \n",
      "                     spark.app.startTime = 1692616535627       \n",
      "                    spark.app.submitTime = 1692616535464       \n",
      "            spark.ui.showConsoleProgress = true                \n",
      "                       spark.driver.port = 42141               \n",
      "         spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/22 09:14:18 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType, Row\n",
    "from pyspark.sql.functions import * # col, array, lit, udf, min, max, round # lit is used for applying one scalar to every row in a whole column when using withColumn and creating a new column\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.param.shared import *\n",
    "from pyspark.mllib.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "# Create a SparkConf object\n",
    "#Be sure that the sum of the driver or executor memory plus the driver or executor memory overhead is always less than the value of yarn.nodemanager.resource.memory-mb \\\n",
    "#spark.driver/executor.memory + spark.driver/executor.memoryOverhead < yarn.nodemanager.resource.memory-mb \\\n",
    "\n",
    "conf = SparkConf().setAppName(\"MergeDatasets\").set(\"spark.executor.memory\", \"20g\") \\\n",
    "                                            .set(\"spark.driver.memory\", \"20g\") \\\n",
    "                                            .set(\"spark.driver.cores\", \"4\") \\\n",
    "                                            .set(\"spark.executor.cores\", \"4\") \\\n",
    "                                            .set(\"spark.driver.memoryOverhead\", \"1024\") \\\n",
    "                                            .set(\"spark.executor.memoryOverhead\", \"1024\") \\\n",
    "                                            .set(\"yarn.nodemanager.resource.memory-mb\", \"196608\") \\\n",
    "                                            .set(\"yarn.nodemanager.vmem-check-enabled\", \"false\") \\\n",
    "                                            .set(\"spark.shuffle.service.enabled\", \"True\") \\\n",
    "                                            .set(\"spark.dynamicAllocation.enabled\", \"True\") \\\n",
    "                                            .set(\"spark.dynamicAllocation.initialExecutors\", \"16\") \\\n",
    "                                            .set(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\") \\\n",
    "                                            .set(\"spark.dynamicAllocation.minExecutors\", \"16\") \\\n",
    "                                            .set(\"spark.dynamicAllocation.maxExecutors\", \"16\") \n",
    "                                            #.set(\"spark.yarn.executor.memoryOverhead\", \"8192\") \n",
    "                                            \n",
    "# Create a SparkSession object\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#repartition_count = 144*100;\n",
    "\n",
    "for option in sc.getConf().getAll():\n",
    "    print( \"{0:>40} = {1:<20}\".format(str(option[0]), str(option[1])) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d35345",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqlContext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15559/1750568431.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mrp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b\\'\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mkv_rp\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv_rp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mrp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrp_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_to_vector_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.repartition(repartition_count)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sqlContext' is not defined"
     ]
    }
   ],
   "source": [
    "pathName = \"SpotiFeat6M/merged\"\n",
    "\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "#########################################################\n",
    "#   Pre- Process RH and RP for Euclidean\n",
    "#\n",
    "rp = sc.textFile(pathName + \".rp\")\n",
    "rp = rp.map(lambda x: x.replace(\"\\\"\",\"\").replace(\"b\\'\",\"\").replace(\"'\",\"\")).map(lambda x: x.split(\",\"))\n",
    "kv_rp= rp.map(lambda x: (x[0].replace(\";\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace(\" \",\"\"), list(x[1:])))\n",
    "rp_df = sqlContext.createDataFrame(kv_rp, [\"id\", \"rp\"])\n",
    "rp_df = rp_df.select(rp_df[\"id\"],list_to_vector_udf(rp_df[\"rp\"]).alias(\"rp\"))#.repartition(repartition_count)\n",
    "\n",
    "rh = sc.textFile(pathName + \".rh\")\n",
    "rh = rh.map(lambda x: x.replace(\"\\\"\",\"\").replace(\"b\\'\",\"\").replace(\"'\",\"\")).map(lambda x: x.split(\",\"))\n",
    "kv_rh= rh.map(lambda x: (x[0].replace(\";\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace(\" \",\"\"), list(x[1:])))\n",
    "rh_df = sqlContext.createDataFrame(kv_rh, [\"id\", \"rh\"])\n",
    "rh_df = rh_df.select(rh_df[\"id\"],list_to_vector_udf(rh_df[\"rh\"]).alias(\"rh\"))#.repartition(repartition_count)\n",
    "\n",
    "#########################################################\n",
    "#   Pre- Process BH for Euclidean\n",
    "#\n",
    "bh = sc.textFile(pathName + \".bh\")\n",
    "bh = bh.map(lambda x: x.split(\";\"))\n",
    "kv_bh = bh.map(lambda x: (x[0].replace(\"/beegfs/ja62lel/6M/\",\"\").replace(\";\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace(\" \",\"\").replace(\"b\\'\",\"\").replace(\"'\",\"\").replace(\"\\\"\",\"\"), x[1], Vectors.dense(x[2].replace(' ', '').replace('[', '').replace(']', '').split(','))))\n",
    "bh_df = sqlContext.createDataFrame(kv_bh, [\"id\", \"bpm\", \"bh\"])#.repartition(repartition_count)\n",
    "#########################################################\n",
    "#   Pre- Process Notes for Levenshtein\n",
    "#\n",
    "notes = sc.textFile(pathName + \".notes\")\n",
    "notes = notes.map(lambda x: x.split(';'))\n",
    "notes = notes.map(lambda x: (x[0].replace(\"/beegfs/ja62lel/6M/\",\"\").replace(\";\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace(\" \",\"\").replace(\"b\\'\",\"\").replace(\"'\",\"\").replace(\"\\\"\",\"\"), x[1], x[2], x[3].replace(\"10\",'K').replace(\"11\",'L').replace(\"0\",'A').replace(\"1\",'B').replace(\"2\",'C').replace(\"3\",'D').replace(\"4\",'E').replace(\"5\",'F').replace(\"6\",'G').replace(\"7\",'H').replace(\"8\",'I').replace(\"9\",'J')))\n",
    "notes = notes.map(lambda x: (x[0], x[1], x[2], x[3].replace(',','').replace(' ','')))\n",
    "notesDf = sqlContext.createDataFrame(notes, [\"id\", \"key\", \"scale\", \"notes\"])#.repartition(repartition_count)\n",
    "#########################################################\n",
    "#   Pre- Process Chroma for cross-correlation\n",
    "#\n",
    "chroma = sc.textFile(pathName + \".chroma\")\n",
    "chroma = chroma.map(lambda x: x.replace(' ', '').replace(';', ','))\n",
    "chroma = chroma.map(lambda x: x.replace('.mp3,', '.mp3;').replace('.wav,', '.wav;').replace('.m4a,', '.m4a;').replace('.aiff,', '.aiff;').replace('.aif,', '.aif;').replace('.au,', '.au;').replace('.flac,', '.flac;').replace('.ogg,', '.ogg;'))\n",
    "chroma = chroma.map(lambda x: x.split(';'))\n",
    "#try to filter out empty elements\n",
    "chroma = chroma.filter(lambda x: (not x[1] == '[]') and (x[1].startswith(\"[[0.\") or x[1].startswith(\"[[1.\")))\n",
    "chromaRdd = chroma.map(lambda x: (x[0].replace(\"/beegfs/ja62lel/6M/\",\"\").replace(\";\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace(\" \",\"\").replace(\"b\\'\",\"\").replace(\"'\",\"\").replace(\"\\\"\",\"\"),(x[1].replace(' ', '').replace('[', '').replace(']', '').split(','))))\n",
    "chromaVec = chromaRdd.map(lambda x: (x[0], Vectors.dense(x[1])))\n",
    "chromaDf = sqlContext.createDataFrame(chromaVec, [\"id\", \"chroma\"])#.repartition(repartition_count)\n",
    "#########################################################\n",
    "#   Pre- Process MFCC for SKL and JS and EUC\n",
    "#\n",
    "mfcc = sc.textFile(pathName + \".mfcckl\")            \n",
    "mfcc = mfcc.map(lambda x: x.replace(' ', '').replace(';', ','))\n",
    "mfcc = mfcc.map(lambda x: x.replace('.mp3,', '.mp3;').replace('.wav,', '.wav;').replace('.m4a,', '.m4a;').replace('.aiff,', '.aiff;').replace('.aif,', '.aif;').replace('.au,', '.au;').replace('.flac,', '.flac;').replace('.ogg,', '.ogg;'))\n",
    "mfcc = mfcc.map(lambda x: x.split(';'))\n",
    "mfcc = mfcc.map(lambda x: (x[0].replace(\"/beegfs/ja62lel/6M/\",\"\").replace(\";\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace(\" \",\"\").replace(\"b\\'\",\"\").replace(\"'\",\"\").replace(\"\\\"\",\"\"), x[1].replace('[', '').replace(']', '').split(',')))\n",
    "mfccVec = mfcc.map(lambda x: (x[0], Vectors.dense(x[1])))\n",
    "mfccDfMerged = sqlContext.createDataFrame(mfccVec, [\"id\", \"mfccSkl\"])#.repartition(repartition_count)\n",
    "\n",
    "#########################################################\n",
    "#   Gather all features in one dataframe\n",
    "#\n",
    "featureDF = chromaDf.join(mfccDfMerged, on=[\"id\"], how='inner')#.persist()\n",
    "featureDF = featureDF.join(notesDf, on=['id'], how='inner')#.persist()\n",
    "featureDF = featureDF.join(rp_df, on=['id'], how='inner')#.persist()\n",
    "featureDF = featureDF.join(rh_df, on=['id'], how='inner')#.persist()\n",
    "featureDF = featureDF.join(bh_df, on=['id'], how='inner').dropDuplicates().persist()\n",
    "#Force lazy evaluation to evaluate with an action\n",
    "#trans = featureDF.count()\n",
    "#print(featureDF.count())\n",
    "#########################################################\n",
    "\n",
    "featureDF.write.json(\"AudioFeatures6MMerged.json\")\n",
    "\n",
    "#########################################################\n",
    "\n",
    "#   DEBUGPRINT UNPERSIST\n",
    "#\t\t\t\t\t\n",
    "chromaDf.unpersist()\n",
    "mfccDfMerged.unpersist()\n",
    "notesDf.unpersist()\n",
    "rp_df.unpersist()\n",
    "rh_df.unpersist()\n",
    "bh_df.unpersist()\n",
    "#########################################################\n",
    "#  16 Nodes, 192GB RAM each, 36 cores each (+ hyperthreading = 72)\n",
    "#   -> max 1152 executors\n",
    "#fullFeatureDF = featureDF.repartition(self.repartition_count).persist()\n",
    "#print(fullFeatureDF.count())\n",
    "#fullFeatureDF.toPandas().to_csv(\"featureDF.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d952de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathName = \"SpotiFeat1_7M/merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf75f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "#   Pre- Process RH and RP for Euclidean\n",
    "#\n",
    "rp = sc.textFile(pathName + \".rp\")\n",
    "rp = rp.map(lambda x: x.replace(\"\\\"\",\"\").replace(\"b\\'\",\"\").replace(\"'\",\"\")).map(lambda x: x.split(\",\"))\n",
    "kv_rp= rp.map(lambda x: (x[0].replace(\";\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace(\" \",\"\"), list(x[1:])))\n",
    "rp_df = sqlContext.createDataFrame(kv_rp, [\"id\", \"rp\"])\n",
    "rp_df = rp_df.select(rp_df[\"id\"],list_to_vector_udf(rp_df[\"rp\"]).alias(\"rp\"))#.repartition(repartition_count)\n",
    "\n",
    "rh = sc.textFile(pathName + \".rh\")\n",
    "rh = rh.map(lambda x: x.replace(\"\\\"\",\"\").replace(\"b\\'\",\"\").replace(\"'\",\"\")).map(lambda x: x.split(\",\"))\n",
    "kv_rh= rh.map(lambda x: (x[0].replace(\";\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace(\" \",\"\"), list(x[1:])))\n",
    "rh_df = sqlContext.createDataFrame(kv_rh, [\"id\", \"rh\"])\n",
    "rh_df = rh_df.select(rh_df[\"id\"],list_to_vector_udf(rh_df[\"rh\"]).alias(\"rh\"))#.repartition(repartition_count)\n",
    "\n",
    "#########################################################\n",
    "#   Pre- Process BH for Euclidean\n",
    "#\n",
    "bh = sc.textFile(pathName + \".bh\")\n",
    "bh = bh.map(lambda x: x.split(\";\"))\n",
    "kv_bh = bh.map(lambda x: (x[0].replace(\"/beegfs/ja62lel/6M/\",\"\").replace(\";\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace(\" \",\"\").replace(\"b\\'\",\"\").replace(\"'\",\"\").replace(\"\\\"\",\"\"), x[1], Vectors.dense(x[2].replace(' ', '').replace('[', '').replace(']', '').split(','))))\n",
    "bh_df = sqlContext.createDataFrame(kv_bh, [\"id\", \"bpm\", \"bh\"])#.repartition(repartition_count)\n",
    "#########################################################\n",
    "#   Pre- Process Notes for Levenshtein\n",
    "#\n",
    "notes = sc.textFile(pathName + \".notes\")\n",
    "notes = notes.map(lambda x: x.split(';'))\n",
    "notes = notes.map(lambda x: (x[0].replace(\"/beegfs/ja62lel/6M/\",\"\").replace(\";\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace(\" \",\"\").replace(\"b\\'\",\"\").replace(\"'\",\"\").replace(\"\\\"\",\"\"), x[1], x[2], x[3].replace(\"10\",'K').replace(\"11\",'L').replace(\"0\",'A').replace(\"1\",'B').replace(\"2\",'C').replace(\"3\",'D').replace(\"4\",'E').replace(\"5\",'F').replace(\"6\",'G').replace(\"7\",'H').replace(\"8\",'I').replace(\"9\",'J')))\n",
    "notes = notes.map(lambda x: (x[0], x[1], x[2], x[3].replace(',','').replace(' ','')))\n",
    "notesDf = sqlContext.createDataFrame(notes, [\"id\", \"key\", \"scale\", \"notes\"])#.repartition(repartition_count)\n",
    "#########################################################\n",
    "#   Pre- Process Chroma for cross-correlation\n",
    "#\n",
    "chroma = sc.textFile(pathName + \".chroma\")\n",
    "chroma = chroma.map(lambda x: x.replace(' ', '').replace(';', ','))\n",
    "chroma = chroma.map(lambda x: x.replace('.mp3,', '.mp3;').replace('.wav,', '.wav;').replace('.m4a,', '.m4a;').replace('.aiff,', '.aiff;').replace('.aif,', '.aif;').replace('.au,', '.au;').replace('.flac,', '.flac;').replace('.ogg,', '.ogg;'))\n",
    "chroma = chroma.map(lambda x: x.split(';'))\n",
    "#try to filter out empty elements\n",
    "chroma = chroma.filter(lambda x: (not x[1] == '[]') and (x[1].startswith(\"[[0.\") or x[1].startswith(\"[[1.\")))\n",
    "chromaRdd = chroma.map(lambda x: (x[0].replace(\"/beegfs/ja62lel/6M/\",\"\").replace(\";\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace(\" \",\"\").replace(\"b\\'\",\"\").replace(\"'\",\"\").replace(\"\\\"\",\"\"),(x[1].replace(' ', '').replace('[', '').replace(']', '').split(','))))\n",
    "chromaVec = chromaRdd.map(lambda x: (x[0], Vectors.dense(x[1])))\n",
    "chromaDf = sqlContext.createDataFrame(chromaVec, [\"id\", \"chroma\"])#.repartition(repartition_count)\n",
    "#########################################################\n",
    "#   Pre- Process MFCC for SKL and JS and EUC\n",
    "#\n",
    "mfcc = sc.textFile(pathName + \".mfcckl\")            \n",
    "mfcc = mfcc.map(lambda x: x.replace(' ', '').replace(';', ','))\n",
    "mfcc = mfcc.map(lambda x: x.replace('.mp3,', '.mp3;').replace('.wav,', '.wav;').replace('.m4a,', '.m4a;').replace('.aiff,', '.aiff;').replace('.aif,', '.aif;').replace('.au,', '.au;').replace('.flac,', '.flac;').replace('.ogg,', '.ogg;'))\n",
    "mfcc = mfcc.map(lambda x: x.split(';'))\n",
    "mfcc = mfcc.map(lambda x: (x[0].replace(\"/beegfs/ja62lel/6M/\",\"\").replace(\";\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace(\" \",\"\").replace(\"b\\'\",\"\").replace(\"'\",\"\").replace(\"\\\"\",\"\"), x[1].replace('[', '').replace(']', '').split(',')))\n",
    "mfccVec = mfcc.map(lambda x: (x[0], Vectors.dense(x[1])))\n",
    "mfccDfMerged = sqlContext.createDataFrame(mfccVec, [\"id\", \"mfccSkl\"])#.repartition(repartition_count)\n",
    "\n",
    "#########################################################\n",
    "# WRITE\n",
    "#########################################################\n",
    "\n",
    "featureDF2 = chromaDf.join(mfccDfMerged, on=[\"id\"], how='inner')#.persist()\n",
    "featureDF2 = featureDF2.join(notesDf, on=['id'], how='inner')#.persist()\n",
    "featureDF2 = featureDF2.join(rp_df, on=['id'], how='inner')#.persist()\n",
    "featureDF2 = featureDF2.join(rh_df, on=['id'], how='inner')#.persist()\n",
    "featureDF2 = featureDF2.join(bh_df, on=['id'], how='inner').dropDuplicates().persist()\n",
    "\n",
    "featureDF2.write.json(\"AudioFeatures1_7MMerged.json\")\n",
    "\n",
    "chromaDf.unpersist()\n",
    "mfccDfMerged.unpersist()\n",
    "notesDf.unpersist()\n",
    "rp_df.unpersist()\n",
    "rh_df.unpersist()\n",
    "bh_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDF2 = featureDF2.join(featureDF, on=[\"id\"], how='inner').dropDuplicates()\n",
    "featureDF2.write.json(\"AudioFeaturesAllMerged.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(\"\", schema=df.schema, nullValue=\"Hyukjin Kwon\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
